{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Advanced_DL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["###Mount Drive"],"metadata":{"id":"iUQ82jLwfCzp"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TH6oy-ElfHev","executionInfo":{"status":"ok","timestamp":1651200828673,"user_tz":240,"elapsed":2506,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}},"outputId":"b336aa29-d982-40a1-e0d7-cdc93cdf3e60"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Since we've shared this drive with you, please use the correct file path from your drive since it'll go to SharedDrive for you\n","%cd '/content/drive/MyDrive/vqa'"],"metadata":{"id":"_IbzjAPGfPrB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651200828673,"user_tz":240,"elapsed":6,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}},"outputId":"2aae3f2b-c936-484d-c4d0-320a109ad732"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/vqa\n"]}]},{"cell_type":"markdown","source":["###Import relevant libraries"],"metadata":{"id":"BeENDeNpLPbb"}},{"cell_type":"code","execution_count":38,"metadata":{"id":"EWPWCQJ9LM7W","executionInfo":{"status":"ok","timestamp":1651200832062,"user_tz":240,"elapsed":2,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torchvision.models as models\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from PIL import Image\n","import re\n","import time\n","import cv2\n","import warnings"]},{"cell_type":"markdown","source":["###Setting the constants and parameter values"],"metadata":{"id":"tBtSbtFJMQ9U"}},{"cell_type":"code","source":["input_dir = '/content/drive/MyDrive/vqa'\n","log_dir = '/content/drive/MyDrive/vqa/logs'\n","model_dir = '/content/drive/MyDrive/vqa/models'\n","\n","# maximum length of question, the length in the VQA dataset is 26\n","max_qst_length = 30\n","# maximum number of answers\n","max_num_ans = 10\n","# embedding size of feature vector for image and question\n","embed_size = 1024\n","# embedding size of the word used as the input for the LSTM\n","word_embed_size = 300\n","# Number of layers in the LSTM\n","num_layers = 2\n","# Hidden size in the LSTM\n","hidden_size = 64\n","# Learning rate, step size and decay rate used while initializing the Step learning rate Scheduler\n","learning_rate = 0.001\n","step_size = 10\n","gamma = 0.1\n","#Number of epochs it is trained on\n","num_epochs = 30\n","#Batch size, number of workers and the steps after which the model parameters are saved\n","batch_size = 256\n","num_workers = 4\n","save_step = 1\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"4kyUxL5EMVqW","executionInfo":{"status":"ok","timestamp":1651200833462,"user_tz":240,"elapsed":4,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["###Helper Functions for Handling Text"],"metadata":{"id":"byQqyFa2LyJ4"}},{"cell_type":"code","source":["SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n","\n","# create tokens\n","def tokenize(sentence):\n","    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n","    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n","    return tokens\n","\n","# returns a file as list of lines\n","def load_str_list(fname):\n","    with open(fname) as f:\n","        lines = f.readlines()\n","    lines = [l.strip() for l in lines]\n","    return lines\n","\n","\n","# Tokenizes the text and then gives the index of the word from the vocab txt file of answers and questions\n","class VocabDict:\n","\n","    def __init__(self, vocab_file):\n","        self.word_list = load_str_list(vocab_file)\n","        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n","        self.vocab_size = len(self.word_list)\n","        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n","\n","    def idx2word(self, n_w):\n","\n","        return self.word_list[n_w]\n","\n","    def word2idx(self, w):\n","        if w in self.word2idx_dict:\n","            return self.word2idx_dict[w]\n","        elif self.unk2idx is not None:\n","            return self.unk2idx\n","        else:\n","            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n","\n","    def tokenize_and_index(self, sentence):\n","        inds = [self.word2idx(w) for w in tokenize(sentence)]\n","\n","        return inds\n","        "],"metadata":{"id":"nO1Aqs0QLsWM","executionInfo":{"status":"ok","timestamp":1651200835463,"user_tz":240,"elapsed":4,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["###Building the Dataset and DataLoader"],"metadata":{"id":"MMKzQIZZMCyh"}},{"cell_type":"code","source":["class VqaDataset(data.Dataset):\n","\n","    def __init__(self, input_dir, input_vqa, max_qst_length=30, max_num_ans=10, transform=None):\n","        self.input_dir = input_dir\n","        self.vqa = np.load(input_dir+'/'+input_vqa, allow_pickle=True)\n","        self.qst_vocab = VocabDict(input_dir+'/dataset/vocab_questions.txt')\n","        self.ans_vocab = VocabDict(input_dir+'/dataset/vocab_answers.txt')\n","        self.max_qst_length = max_qst_length\n","        self.max_num_ans = max_num_ans\n","        self.load_ans = ('valid_answers' in self.vqa[0]) and (self.vqa[0]['valid_answers'] is not None)\n","        self.transform = transform\n","\n","    def __getitem__(self, idx):\n","\n","        vqa = self.vqa\n","        qst_vocab = self.qst_vocab\n","        ans_vocab = self.ans_vocab\n","        max_qst_length = self.max_qst_length\n","        max_num_ans = self.max_num_ans\n","        transform = self.transform\n","        load_ans = self.load_ans\n","\n","        image = vqa[idx]['image_path']\n","        image = Image.open(image).convert('RGB')\n","        qst2idc = np.array([qst_vocab.word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n","        qst2idc[:len(vqa[idx]['question_tokens'])] = [qst_vocab.word2idx(w) for w in vqa[idx]['question_tokens']]\n","        sample = {'image': image, 'question': qst2idc}\n","\n","        if load_ans:\n","            ans2idc = [ans_vocab.word2idx(w) for w in vqa[idx]['valid_answers']]\n","            ans2idx = np.random.choice(ans2idc)\n","            sample['answer_label'] = ans2idx         # for training\n","\n","            mul2idc = list([-1] * max_num_ans)       # padded with -1 (no meaning) not used in 'ans_vocab'\n","            mul2idc[:len(ans2idc)] = ans2idc         # our model should not predict -1\n","            sample['answer_multi_choice'] = mul2idc  # for evaluation metric\n","\n","        if transform:\n","            sample['image'] = transform(sample['image'])\n","\n","        return sample\n","\n","    def __len__(self):\n","\n","        return len(self.vqa)\n","\n","\n","def get_loader(input_dir, input_vqa_train, input_vqa_valid, max_qst_length, max_num_ans, batch_size, num_workers):\n","\n","    transform = {\n","        phase: transforms.Compose([transforms.ToTensor(),\n","                                   transforms.Normalize((0.485, 0.456, 0.406),\n","                                                        (0.229, 0.224, 0.225))]) \n","        for phase in ['train', 'valid']}\n","\n","    vqa_dataset = {\n","        'train': VqaDataset(\n","            input_dir=input_dir,\n","            input_vqa=input_vqa_train,\n","            max_qst_length=max_qst_length,\n","            max_num_ans=max_num_ans,\n","            transform=transform['train']),\n","        'valid': VqaDataset(\n","            input_dir=input_dir,\n","            input_vqa=input_vqa_valid,\n","            max_qst_length=max_qst_length,\n","            max_num_ans=max_num_ans,\n","            transform=transform['valid'])}\n","\n","    data_loader = {\n","        phase: torch.utils.data.DataLoader(\n","            dataset=vqa_dataset[phase],\n","            batch_size=batch_size,\n","            shuffle=True,\n","            num_workers=num_workers)\n","        for phase in ['train', 'valid']}\n","\n","    return data_loader"],"metadata":{"id":"j2ri8KxPL3xH","executionInfo":{"status":"ok","timestamp":1651200838572,"user_tz":240,"elapsed":329,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["##Model: Image Encoding Block"],"metadata":{"id":"_kubGfT_R0yQ"}},{"cell_type":"code","source":["class ImgAttentionEncoder(nn.Module):\n","\n","    def __init__(self, embed_size):\n","\n","        super(ImgAttentionEncoder, self).__init__()\n","        vggnet_feat = models.vgg19(pretrained=True).features\n","        modules = list(vggnet_feat.children())[:-2]\n","        self.cnn = nn.Sequential(*modules)\n","        self.fc = nn.Sequential(nn.Linear(self.cnn[-3].out_channels, embed_size),\n","                                nn.Tanh())     # feature vector of image\n","\n","    def forward(self, image):\n","\n","        with torch.no_grad():\n","            img_feature = self.cnn(image)    \n","        #print(img_feature.shape)                       # [batch_size, vgg16(19)_fc=4096]\n","        img_feature = img_feature.view(-1, 512, 196).transpose(1,2) # [batch_size, 196, 512]\n","        img_feature = self.fc(img_feature)                          # [batch_size, 196, embed_size]\n","        #print(image.shape, img_feature.shape,\"\\n\")\n","\n","        return img_feature\n","\n"],"metadata":{"id":"3crpWchKR1Ua","executionInfo":{"status":"ok","timestamp":1651200842039,"user_tz":240,"elapsed":341,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["###Model: Question Encoding Block"],"metadata":{"id":"RS-9L7-_SmpP"}},{"cell_type":"code","source":["class QstEncoder(nn.Module):\n","\n","    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n","\n","        super(QstEncoder, self).__init__()\n","        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n","        self.tanh = nn.Tanh()\n","        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n","        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)     # 2 for hidden and cell states\n","\n","    def forward(self, question):\n","\n","        qst_vec = self.word2vec(question)                             # [batch_size, max_qst_length=30, word_embed_size=300]\n","        qst_vec = self.tanh(qst_vec)\n","        qst_vec = qst_vec.transpose(0, 1)                             # [max_qst_length=30, batch_size, word_embed_size=300]\n","        _, (hidden, cell) = self.lstm(qst_vec)                        # [num_layers=2, batch_size, hidden_size=512]\n","        qst_feature = torch.cat((hidden, cell), 2)                    # [num_layers=2, batch_size, 2*hidden_size=1024]\n","        qst_feature = qst_feature.transpose(0, 1)                     # [batch_size, num_layers=2, 2*hidden_size=1024]\n","        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1)  # [batch_size, 2*num_layers*hidden_size=2048]\n","        qst_feature = self.tanh(qst_feature)\n","        qst_feature = self.fc(qst_feature)                            # [batch_size, embed_size]\n","\n","        return qst_feature"],"metadata":{"id":"InpvRJdcSqPD","executionInfo":{"status":"ok","timestamp":1651200844822,"user_tz":240,"elapsed":364,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["###Model: Attention Block"],"metadata":{"id":"eAxCHSplZyvG"}},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, num_channels, embed_size, dropout=True):\n","\n","        super(Attention, self).__init__()\n","        self.ff_image = nn.Linear(embed_size, num_channels)\n","        self.ff_questions = nn.Linear(embed_size, num_channels)\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.ff_attention = nn.Linear(num_channels, 1)\n","\n","    def forward(self, vi, vq):\n","\n","        hi = self.ff_image(vi)\n","        hq = self.ff_questions(vq).unsqueeze(dim=1)\n","        ha = torch.tanh(hi+hq)\n","        if self.dropout:\n","            ha = self.dropout(ha)\n","        ha = self.ff_attention(ha)\n","        pi = torch.softmax(ha, dim=1)\n","        self.pi = pi\n","        vi_attended = (pi * vi).sum(dim=1)    # creating the weighted image vector using attention distribution\n","        u = vi_attended + vq      # concatenating the new query vector and the weighted image vector\n","        return u"],"metadata":{"id":"mTUQNgeSZ1ZG","executionInfo":{"status":"ok","timestamp":1651200847410,"user_tz":240,"elapsed":475,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["###Model: Combine Image Encoding and Question Encoding Block"],"metadata":{"id":"NIFmwwSJS_9s"}},{"cell_type":"code","source":["class SANModel(nn.Module):\n","    def __init__(self, embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size): \n","        super(SANModel, self).__init__()\n","        self.num_attention_layer = 2\n","        self.num_mlp_layer = 1\n","        self.img_encoder = ImgAttentionEncoder(embed_size)\n","        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n","        self.san = nn.ModuleList([Attention(512, embed_size)]*self.num_attention_layer)\n","        self.tanh = nn.Tanh()\n","        self.mlp = nn.Sequential(nn.Dropout(p=0.5),\n","                            nn.Linear(embed_size, ans_vocab_size))\n","        self.attn_features = []  ## attention features\n","\n","    def forward(self, img, qst):\n","\n","        img_feature = self.img_encoder(img)                     # [batch_size, embed_size]\n","        qst_feature = self.qst_encoder(qst)                     # [batch_size, embed_size]\n","        vi = img_feature\n","        u = qst_feature\n","        for attn_layer in self.san:\n","            u = attn_layer(vi, u)\n","            \n","        combined_feature = self.mlp(u)\n","        return combined_feature"],"metadata":{"id":"C57q-LSBTE9g","executionInfo":{"status":"ok","timestamp":1651200851039,"user_tz":240,"elapsed":2282,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["###Training Loop"],"metadata":{"id":"Z2JJKjmqPBTM"}},{"cell_type":"code","source":["# Get data loader for train and test - it's a dictionary with the key train having the train dataloader and same for test\n","data_loader = get_loader(\n","        input_dir=input_dir,\n","        input_vqa_train='dataset/train.npy',\n","        input_vqa_valid='dataset/valid.npy',\n","        max_qst_length=max_qst_length,\n","        max_num_ans=max_num_ans,\n","        batch_size=batch_size,\n","        num_workers=num_workers)\n","\n","qst_vocab_size = data_loader['train'].dataset.qst_vocab.vocab_size\n","ans_vocab_size = data_loader['train'].dataset.ans_vocab.vocab_size\n","ans_unk_idx = data_loader['train'].dataset.ans_vocab.unk2idx\n","\n","# Initializing the model\n","model = SANModel(\n","        embed_size=embed_size,\n","        qst_vocab_size=qst_vocab_size,\n","        ans_vocab_size=ans_vocab_size,\n","        word_embed_size=word_embed_size,\n","        num_layers=num_layers,\n","        hidden_size=hidden_size).to(device)\n","\n","# Initializing the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Initializing the optimizer and learning rate scheduler\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","\n","\n","last_time = 0\n","early_stop_threshold = 3\n","best_loss = 99999\n","val_increase_count = 0\n","stop_training = False\n","prev_loss = 9999\n","\n","for epoch in range(num_epochs):\n","\n","    for phase in ['train', 'valid']:\n","\n","        running_loss = 0.0\n","        running_corr = 0\n","\n","        batch_step_size = len(data_loader[phase].dataset) / batch_size\n","\n","        if phase == 'train':\n","            scheduler.step()\n","            model.train()\n","        else:\n","            model.eval()\n","\n","        for batch_idx, batch_sample in enumerate(data_loader[phase]):\n","            \n","            image = batch_sample['image'].to(device)\n","            question = batch_sample['question'].to(device)\n","            label = batch_sample['answer_label'].to(device)\n","            multi_choice = batch_sample['answer_multi_choice']  # not tensor, list.\n","\n","            optimizer.zero_grad()\n","\n","            with torch.set_grad_enabled(phase == 'train'):\n","\n","                output = model(image, question)      # size: [batch_size X ans_vocab_size=1000]\n","                _, pred = torch.max(output, 1)  # size: [batch_size]\n","\n","                loss = criterion(output, label)\n","\n","                if phase == 'train':\n","                    loss.backward()\n","                    optimizer.step()\n","\n","            # Evaluation metric \n","            running_loss += loss.item()\n","            running_corr += torch.stack([(ans == pred.cpu()) for ans in multi_choice]).any(dim=0).sum()\n","\n","            # Print the average loss in a mini-batch.\n","            if batch_idx % 10 == 0:\n","                time_taken = time.time() - last_time\n","                time_left = (((batch_step_size - batch_idx) * time_taken)/10) * (num_epochs - epoch)\n","                print('| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}, Time left: {:.2f} hr'\n","                      .format(phase.upper(), epoch+1, num_epochs, batch_idx, int(batch_step_size), loss.item(), time_left/3600))\n","                last_time = time.time()\n","        # Print the average loss and accuracy in an epoch.\n","        epoch_loss = running_loss / batch_step_size\n","        epoch_acc = running_corr.double() / len(data_loader[phase].dataset)      \n","\n","        print('| {} SET | Epoch [{:02d}/{:02d}], Loss: {:.4f}, Acc: {:.4f}\\n'\n","              .format(phase.upper(), epoch+1, num_epochs, epoch_loss, epoch_acc))\n","\n","        \n","\n","        # Log the loss and accuracy in an epoch.\n","        with open(os.path.join(log_dir, '{}-log-epoch-{:02}.txt')\n","                  .format(phase, epoch+1), 'w') as f:\n","            f.write(str(epoch+1) + '\\t'\n","                    + str(epoch_loss) + '\\t'\n","                    + str(epoch_acc.item()))\n","            \n","        if phase == 'valid':\n","            if epoch_loss < best_loss:\n","                best_loss = epoch_loss\n","                torch.save(model, os.path.join(model_dir, 'best_model.pt'))\n","            if epoch_loss > prev_loss:\n","                val_increase_count += 1\n","            else:\n","                val_increase_count = 0\n","            if val_increase_count >= early_stop_threshold:\n","                stop_training = True\n","            prev_loss = epoch_loss\n","\n","    # Save the model check points.\n","    if (epoch+1) % save_step == 0:\n","        torch.save(model, os.path.join(model_dir, '-epoch-{:02d}.pt'.format(epoch+1)))"],"metadata":{"id":"auJoIXFiPAhE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Inference"],"metadata":{"id":"Ya37IFIqTca8"}},{"cell_type":"code","source":["image_path = '/content/drive/MyDrive/vqa/dataset/Resized_Images/test_img.jpeg'\n","question = 'what does the sign say?'\n","saved_model = '/content/drive/MyDrive/vqa/models/best_model.pt'\n","max_qst_length=30"],"metadata":{"id":"-v5XEy_EUs2U","executionInfo":{"status":"ok","timestamp":1651200896403,"user_tz":240,"elapsed":3,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["warnings.filterwarnings(\"ignore\")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","qst_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_questions.txt\")\n","ans_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_answers.txt\")\n","word2idx_dict = {w:n_w for n_w, w in enumerate(qst_vocab)}\n","unk2idx = word2idx_dict['<unk>'] if '<unk>' in word2idx_dict else None\n","qst_vocab_size = len(qst_vocab)\n","ans_vocab_size = len(ans_vocab)\n","\n","\n","def word2idx(w):\n","        if w in word2idx_dict:\n","            return word2idx_dict[w]\n","        elif unk2idx is not None:\n","            return unk2idx\n","        else:\n","            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n","\n","image = cv2.imread(image_path)\n","image = cv2.resize(image, dsize=(224,224), interpolation = cv2.INTER_AREA)\n","image = torch.from_numpy(image).float()\n","image = image.to(device)\n","image = image.unsqueeze(dim=0)\n","image = image.view(1,3,224,224)\n","max_qst_length=30\n","\n","try:\n","  q_list = list(question.split(\" \"))\n","except:\n","  q_list = list(question.split(1))\n","idx = 'valid'\n","qst2idc = np.array([word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n","qst2idc[:len(q_list)] = [word2idx(w) for w in q_list]\n","\n","question = qst2idc\n","question = torch.from_numpy(question).long()\n","\n","question = question.to(device)\n","question = question.unsqueeze(dim=0)\n","\n","net = torch.load(saved_model)\n","net = net.to(device)\n","\n","net.eval()\n","output = model(image, question)\n","\n","\n","predicts = torch.softmax(output, 1)\n","probs, indices = torch.topk(predicts, k=5, dim=1)\n","probs = probs.squeeze()\n","indices = indices.squeeze()\n","print(\"predicted - probabilty\")\n","for i in range(5):\n","\n","  print(\"'{}' - {:.4f}\".format(ans_vocab[indices[i].item()], probs[i].item()))"],"metadata":{"id":"aQvOBxL4T1BW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/vqa/'"],"metadata":{"id":"ARUDHmwAD9kZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651201248162,"user_tz":240,"elapsed":369,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}},"outputId":"f5cd96e6-ccd6-4925-d984-06b83db47c9b"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/vqa\n"]}]},{"cell_type":"code","source":["# Running inference on the command line\n","!python /content/drive/MyDrive/vqa/test.py --image_path /content/drive/MyDrive/vqa/dataset/Resized_Images/test_img.jpeg --question 'what does the sign say?' --saved_model /content/drive/MyDrive/vqa/models/best_model.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XszGr0fvEKOc","executionInfo":{"status":"ok","timestamp":1651201258438,"user_tz":240,"elapsed":6159,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}},"outputId":"9f1e618f-07b9-4fe0-d63b-cb99175d09ee"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["predicted - probabilty\n","'<unk>' - 0.1570\n","'stop' - 0.0508\n","'circle' - 0.0488\n","'apple' - 0.0449\n","'fall' - 0.0440\n"]}]}]}