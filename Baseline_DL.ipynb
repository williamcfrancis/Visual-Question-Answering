{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_DL.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMoNaHH+N8QKFc63tRL/Ngp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["###Mount Drive"],"metadata":{"id":"5RZIXFGgfZxL"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tZbqwCjfUvy","executionInfo":{"status":"ok","timestamp":1651201021527,"user_tz":240,"elapsed":1148,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}},"outputId":"1dfb26bc-dfbd-4e39-9c67-d09179beb647"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Since we've shared this drive with you, please use the correct file path from your drive since it'll go to SharedDrive for you\n","%cd '/content/drive/MyDrive/vqa'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFCGeGSkfVrA","executionInfo":{"status":"ok","timestamp":1651201022794,"user_tz":240,"elapsed":181,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}},"outputId":"2e9560c0-5be2-42cb-ecf2-1b1ac433f657"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/vqa\n"]}]},{"cell_type":"markdown","source":["###Import relevant libraries"],"metadata":{"id":"BeENDeNpLPbb"}},{"cell_type":"code","execution_count":69,"metadata":{"id":"EWPWCQJ9LM7W","executionInfo":{"status":"ok","timestamp":1651201024755,"user_tz":240,"elapsed":170,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torchvision.models as models\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from PIL import Image\n","import re\n","import time\n","import cv2\n","import warnings"]},{"cell_type":"markdown","source":["###Setting the constants and parameter values"],"metadata":{"id":"tBtSbtFJMQ9U"}},{"cell_type":"code","source":["input_dir = '/content/drive/MyDrive/vqa'\n","log_dir = '/content/drive/MyDrive/vqa/logs'\n","model_dir = '/content/drive/MyDrive/vqa/models'\n","# maximum length of question, the length in the VQA dataset is 26\n","max_qst_length = 30\n","# maximum number of answers\n","max_num_ans = 10\n","# embedding size of feature vector for image and question\n","embed_size = 1024\n","# embedding size of the word used as the input for the LSTM\n","word_embed_size = 300\n","# Number of layers in the LSTM\n","num_layers = 2\n","# Hidden size in the LSTM\n","hidden_size = 64\n","# Learning rate, step size and decay rate used while initializing the Step learning rate Scheduler\n","learning_rate = 0.001\n","step_size = 10\n","gamma = 0.1\n","#Number of epochs it is trained on\n","num_epochs = 30\n","#Batch size, number of workers and the steps after which the model parameters are saved\n","batch_size = 256\n","num_workers = 4\n","save_step = 1\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"4kyUxL5EMVqW","executionInfo":{"status":"ok","timestamp":1651201027549,"user_tz":240,"elapsed":153,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["###Helper Functions for Handling Text"],"metadata":{"id":"byQqyFa2LyJ4"}},{"cell_type":"code","source":["SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n","\n","# create tokens\n","def tokenize(sentence):\n","    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n","    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n","    return tokens\n","\n","# returns a file as list of lines\n","def load_str_list(fname):\n","    with open(fname) as f:\n","        lines = f.readlines()\n","    lines = [l.strip() for l in lines]\n","    return lines\n","\n","\n","# Tokenizes the text and then gives the index of the word from the vocab txt file of answers and questions\n","class VocabDict:\n","\n","    def __init__(self, vocab_file):\n","        self.word_list = load_str_list(vocab_file)\n","        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n","        self.vocab_size = len(self.word_list)\n","        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n","\n","    def idx2word(self, n_w):\n","\n","        return self.word_list[n_w]\n","\n","    def word2idx(self, w):\n","        if w in self.word2idx_dict:\n","            return self.word2idx_dict[w]\n","        elif self.unk2idx is not None:\n","            return self.unk2idx\n","        else:\n","            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n","\n","    def tokenize_and_index(self, sentence):\n","        inds = [self.word2idx(w) for w in tokenize(sentence)]\n","\n","        return inds"],"metadata":{"id":"nO1Aqs0QLsWM","executionInfo":{"status":"ok","timestamp":1651201031772,"user_tz":240,"elapsed":3,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":["###Building the Dataset and DataLoader"],"metadata":{"id":"MMKzQIZZMCyh"}},{"cell_type":"code","source":["class VqaDataset(data.Dataset):\n","\n","    def __init__(self, input_dir, input_vqa, max_qst_length=30, max_num_ans=10, transform=None):\n","        self.input_dir = input_dir\n","        self.vqa = np.load(input_dir+'/'+input_vqa, allow_pickle=True)\n","        self.qst_vocab = VocabDict(input_dir+'/dataset/vocab_questions.txt')\n","        self.ans_vocab = VocabDict(input_dir+'/dataset/vocab_answers.txt')\n","        self.max_qst_length = max_qst_length\n","        self.max_num_ans = max_num_ans\n","        self.load_ans = ('valid_answers' in self.vqa[0]) and (self.vqa[0]['valid_answers'] is not None)\n","        self.transform = transform\n","\n","    def __getitem__(self, idx):\n","\n","        vqa = self.vqa\n","        qst_vocab = self.qst_vocab\n","        ans_vocab = self.ans_vocab\n","        max_qst_length = self.max_qst_length\n","        max_num_ans = self.max_num_ans\n","        transform = self.transform\n","        load_ans = self.load_ans\n","\n","        image = vqa[idx]['image_path']\n","        image = Image.open(image).convert('RGB')\n","        qst2idc = np.array([qst_vocab.word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n","        qst2idc[:len(vqa[idx]['question_tokens'])] = [qst_vocab.word2idx(w) for w in vqa[idx]['question_tokens']]\n","        sample = {'image': image, 'question': qst2idc}\n","\n","        if load_ans:\n","            ans2idc = [ans_vocab.word2idx(w) for w in vqa[idx]['valid_answers']]\n","            ans2idx = np.random.choice(ans2idc)\n","            sample['answer_label'] = ans2idx         # for training\n","\n","            mul2idc = list([-1] * max_num_ans)       # padded with -1 (no meaning) not used in 'ans_vocab'\n","            mul2idc[:len(ans2idc)] = ans2idc         # our model should not predict -1\n","            sample['answer_multi_choice'] = mul2idc  # for evaluation metric\n","\n","        if transform:\n","            sample['image'] = transform(sample['image'])\n","\n","        return sample\n","\n","    def __len__(self):\n","\n","        return len(self.vqa)\n","\n","\n","def get_loader(input_dir, input_vqa_train, input_vqa_valid, max_qst_length, max_num_ans, batch_size, num_workers):\n","\n","    transform = {\n","        phase: transforms.Compose([transforms.ToTensor(),\n","                                   transforms.Normalize((0.485, 0.456, 0.406),\n","                                                        (0.229, 0.224, 0.225))]) \n","        for phase in ['train', 'valid']}\n","\n","    vqa_dataset = {\n","        'train': VqaDataset(\n","            input_dir=input_dir,\n","            input_vqa=input_vqa_train,\n","            max_qst_length=max_qst_length,\n","            max_num_ans=max_num_ans,\n","            transform=transform['train']),\n","        'valid': VqaDataset(\n","            input_dir=input_dir,\n","            input_vqa=input_vqa_valid,\n","            max_qst_length=max_qst_length,\n","            max_num_ans=max_num_ans,\n","            transform=transform['valid'])}\n","\n","    data_loader = {\n","        phase: torch.utils.data.DataLoader(\n","            dataset=vqa_dataset[phase],\n","            batch_size=batch_size,\n","            shuffle=True,\n","            num_workers=num_workers)\n","        for phase in ['train', 'valid']}\n","\n","    return data_loader"],"metadata":{"id":"j2ri8KxPL3xH","executionInfo":{"status":"ok","timestamp":1651201037809,"user_tz":240,"elapsed":155,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":["###Model: Image Encoding Block"],"metadata":{"id":"_kubGfT_R0yQ"}},{"cell_type":"code","source":["#Block 1: Image Channel which creates the image embedding\n","class ImgEncoder(nn.Module):\n","\n","    def __init__(self, embed_size):\n","        super(ImgEncoder, self).__init__()\n","        model = models.vgg19(pretrained=True)\n","        in_features = model.classifier[-1].in_features  # input size of feature vector\n","        model.classifier = nn.Sequential(\n","            *list(model.classifier.children())[:-1])    # remove last fc layer\n","\n","        self.model = model                              # loaded model without last fc layer\n","        self.fc = nn.Linear(in_features, embed_size)    # feature vector of image\n","\n","    def forward(self, image):\n","\n","        with torch.no_grad():\n","            img_feature = self.model(image)                  # [batch_size, vgg16(19)_fc=4096]\n","        img_feature = self.fc(img_feature)                   # [batch_size, embed_size]\n","\n","        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n","        img_feature = img_feature.div(l2_norm)               # l2-normalized feature vector\n","\n","        return img_feature"],"metadata":{"id":"3crpWchKR1Ua","executionInfo":{"status":"ok","timestamp":1651201043963,"user_tz":240,"elapsed":229,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["###Model: Question Encoding Block"],"metadata":{"id":"RS-9L7-_SmpP"}},{"cell_type":"code","source":["class QstEncoder(nn.Module):\n","\n","    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n","\n","        super(QstEncoder, self).__init__()\n","        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n","        self.tanh = nn.Tanh()\n","        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n","        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)     # 2 for hidden and cell states\n","\n","    def forward(self, question):\n","\n","        qst_vec = self.word2vec(question)                             # [batch_size, max_qst_length=30, word_embed_size=300]\n","        qst_vec = self.tanh(qst_vec)\n","        qst_vec = qst_vec.transpose(0, 1)                             # [max_qst_length=30, batch_size, word_embed_size=300]\n","        _, (hidden, cell) = self.lstm(qst_vec)                        # [num_layers=2, batch_size, hidden_size=512]\n","        qst_feature = torch.cat((hidden, cell), 2)                    # [num_layers=2, batch_size, 2*hidden_size=1024]\n","        qst_feature = qst_feature.transpose(0, 1)                     # [batch_size, num_layers=2, 2*hidden_size=1024]\n","        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1)  # [batch_size, 2*num_layers*hidden_size=2048]\n","        qst_feature = self.tanh(qst_feature)\n","        qst_feature = self.fc(qst_feature)                            # [batch_size, embed_size]\n","\n","        return qst_feature"],"metadata":{"id":"InpvRJdcSqPD","executionInfo":{"status":"ok","timestamp":1651201049229,"user_tz":240,"elapsed":154,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":["###Model: Combine Image Encoding and Question Encoding Block"],"metadata":{"id":"NIFmwwSJS_9s"}},{"cell_type":"code","source":["class VqaModel(nn.Module):\n","\n","    def __init__(self, embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size):\n","\n","        super(VqaModel, self).__init__()\n","        self.img_encoder = ImgEncoder(embed_size)\n","        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n","        self.tanh = nn.Tanh()\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(embed_size, ans_vocab_size)\n","        self.fc2 = nn.Linear(ans_vocab_size, ans_vocab_size)\n","\n","    def forward(self, img, qst):\n","\n","        img_feature = self.img_encoder(img)                     # [batch_size, embed_size]\n","        qst_feature = self.qst_encoder(qst)                     # [batch_size, embed_size]\n","        \n","        # Elementwise multiplication of image and question vectors for fusion\n","        combined_feature = torch.mul(img_feature, qst_feature)  # [batch_size, embed_size]\n","        combined_feature = self.tanh(combined_feature)\n","        combined_feature = self.dropout(combined_feature)\n","        combined_feature = self.fc1(combined_feature)           # [batch_size, ans_vocab_size=1000]\n","        combined_feature = self.tanh(combined_feature)\n","        combined_feature = self.dropout(combined_feature)\n","        combined_feature = self.fc2(combined_feature)           # [batch_size, ans_vocab_size=1000]\n","\n","        return combined_feature"],"metadata":{"id":"C57q-LSBTE9g","executionInfo":{"status":"ok","timestamp":1651201052677,"user_tz":240,"elapsed":164,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["###Training Loop"],"metadata":{"id":"Z2JJKjmqPBTM"}},{"cell_type":"code","source":["# Get data loader for train and test - it's a dictionary with the key train having the train dataloader and same for test\n","data_loader = get_loader(\n","        input_dir=input_dir,\n","        input_vqa_train='dataset/train.npy',\n","        input_vqa_valid='dataset/valid.npy',\n","        max_qst_length=max_qst_length,\n","        max_num_ans=max_num_ans,\n","        batch_size=batch_size,\n","        num_workers=num_workers)\n","\n","qst_vocab_size = data_loader['train'].dataset.qst_vocab.vocab_size\n","ans_vocab_size = data_loader['train'].dataset.ans_vocab.vocab_size\n","ans_unk_idx = data_loader['train'].dataset.ans_vocab.unk2idx\n","\n","# Initializing the model\n","model = VqaModel(\n","        embed_size=embed_size,\n","        qst_vocab_size=qst_vocab_size,\n","        ans_vocab_size=ans_vocab_size,\n","        word_embed_size=word_embed_size,\n","        num_layers=num_layers,\n","        hidden_size=hidden_size).to(device)\n","\n","# Initializing the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Choosing which parameters to update in the optimizer\n","params = list(model.img_encoder.fc.parameters()) \\\n","      + list(model.qst_encoder.parameters()) \\\n","      + list(model.fc1.parameters()) \\\n","      + list(model.fc2.parameters())\n","\n","# Initializing the optimizer and learning rate scheduler\n","optimizer = optim.Adam(params, lr=learning_rate)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","last_time = 0\n","\n","for epoch in range(num_epochs):\n","\n","    for phase in ['train', 'valid']:\n","\n","        running_loss = 0.0\n","        running_corr = 0\n","\n","        batch_step_size = len(data_loader[phase].dataset) / batch_size\n","\n","        if phase == 'train':\n","            scheduler.step()\n","            model.train()\n","        else:\n","            model.eval()\n","\n","        for batch_idx, batch_sample in enumerate(data_loader[phase]):\n","            \n","            image = batch_sample['image'].to(device)\n","            question = batch_sample['question'].to(device)\n","            label = batch_sample['answer_label'].to(device)\n","            multi_choice = batch_sample['answer_multi_choice']  # not tensor, list.\n","\n","            optimizer.zero_grad()\n","\n","            with torch.set_grad_enabled(phase == 'train'):\n","\n","                output = model(image, question)      # size: [batch_size X ans_vocab_size=1000]\n","                _, pred = torch.max(output, 1)  # size: [batch_size]\n","\n","                loss = criterion(output, label)\n","\n","                if phase == 'train':\n","                    loss.backward()\n","                    optimizer.step()\n","\n","            # Evaluation metric \n","            running_loss += loss.item()\n","            running_corr += torch.stack([(ans == pred.cpu()) for ans in multi_choice]).any(dim=0).sum()\n","\n","            # Print the average loss in a mini-batch.\n","            if batch_idx % 10 == 0:\n","                time_taken = time.time() - last_time\n","                time_left = (((batch_step_size - batch_idx) * time_taken)/10) * (num_epochs - epoch)\n","                print('| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}, Time left: {:.2f} hr'\n","                      .format(phase.upper(), epoch+1, num_epochs, batch_idx, int(batch_step_size), loss.item(), time_left/3600))\n","                last_time = time.time()\n","        # Print the average loss and accuracy in an epoch.\n","        epoch_loss = running_loss / batch_step_size\n","        epoch_acc = running_corr.double() / len(data_loader[phase].dataset)      \n","\n","        print('| {} SET | Epoch [{:02d}/{:02d}], Loss: {:.4f}, Acc: {:.4f}\\n'\n","              .format(phase.upper(), epoch+1, num_epochs, epoch_loss, epoch_acc))\n","\n","        \n","\n","        # Log the loss and accuracy in an epoch.\n","        with open(os.path.join(log_dir, '{}-log-epoch-{:02}.txt')\n","                  .format(phase, epoch+1), 'w') as f:\n","            f.write(str(epoch+1) + '\\t'\n","                    + str(epoch_loss) + '\\t'\n","                    + str(epoch_acc.item()))\n","            \n","\n","    # Save the model check points.\n","    if (epoch+1) % save_step == 0:\n","        torch.save(model, os.path.join(model_dir, '-epoch-{:02d}.pt'.format(epoch+1)))"],"metadata":{"id":"auJoIXFiPAhE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Inference"],"metadata":{"id":"Ya37IFIqTca8"}},{"cell_type":"code","source":["image_path = '/content/drive/MyDrive/vqa/dataset/Resized_Images/test_img.jpeg'\n","question = 'What does the sign say?'\n","saved_model = '/content/drive/MyDrive/vqa/models/best_modelb.pt'\n","max_qst_length=30"],"metadata":{"id":"-v5XEy_EUs2U","executionInfo":{"status":"ok","timestamp":1651201087746,"user_tz":240,"elapsed":144,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["warnings.filterwarnings(\"ignore\")\n","\n","qst_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_questions.txt\")\n","ans_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_answers.txt\")\n","word2idx_dict = {w:n_w for n_w, w in enumerate(qst_vocab)}\n","unk2idx = word2idx_dict['<unk>'] if '<unk>' in word2idx_dict else None\n","qst_vocab_size = len(qst_vocab)\n","ans_vocab_size = len(ans_vocab)\n","\n","def word2idx(w):\n","        if w in word2idx_dict:\n","            return word2idx_dict[w]\n","        elif unk2idx is not None:\n","            return unk2idx\n","        else:\n","            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n","\n","image = cv2.imread(image_path)\n","image = cv2.resize(image, dsize=(224,224), interpolation = cv2.INTER_AREA)\n","image = torch.from_numpy(image).float()\n","image = image.to(device)\n","image = image.unsqueeze(dim=0)\n","image = image.view(1,3,224,224)\n","\n","try:\n","  q_list = list(question.split(\" \"))\n","except:\n","  q_list = list(question.split(1))\n","\n","idx = 'valid'\n","qst2idc = np.array([word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n","qst2idc[:len(q_list)] = [word2idx(w) for w in q_list]\n","\n","question = qst2idc\n","question = torch.from_numpy(question).long()\n","\n","question = question.to(device)\n","question = question.unsqueeze(dim=0)\n","\n","net = torch.load(saved_model)\n","net = net.to(device)\n","\n","net.eval()\n","\n","output = net(image, question)\n","predicts = torch.softmax(output, 1)\n","probs, indices = torch.topk(predicts, k=5, dim=1)\n","probs = probs.squeeze()\n","indices = indices.squeeze()\n","print(\"predicted - probabilty\")\n","for i in range(5):\n","    print(\"'{}' - {:.4f}\".format(ans_vocab[indices[i].item()], probs[i].item()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQvOBxL4T1BW","executionInfo":{"status":"ok","timestamp":1651201091377,"user_tz":240,"elapsed":1701,"user":{"displayName":"Diene Newrone","userId":"11499617903453027359"}},"outputId":"2118f5a8-fc1e-4a70-c5a6-1bdd20b5c53f"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["predicted - probabilty\n","'yes' - 0.3498\n","'no' - 0.2768\n","'green' - 0.0493\n","'4' - 0.0321\n","'1' - 0.0250\n"]}]}]}